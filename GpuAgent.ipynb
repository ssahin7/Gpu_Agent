{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec923832-d40d-42ea-8cfd-9de71eddb74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pendulum\n",
      "  Downloading pendulum-3.1.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6 in d:\\python\\lib\\site-packages (from pendulum) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2020.1 in d:\\python\\lib\\site-packages (from pendulum) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.6->pendulum) (1.17.0)\n",
      "Downloading pendulum-3.1.0-cp313-cp313-win_amd64.whl (260 kB)\n",
      "Installing collected packages: pendulum\n",
      "Successfully installed pendulum-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f419017a-fbec-4a8c-b070-4d6865280c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\python\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.2/11.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.5/11.5 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.0/11.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.1/11.5 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.9/11.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.5 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1335898a-5472-4f84-9a6d-0d3bcdcbba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\python\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.1 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.2/11.1 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.1 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/41.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/41.0 MB 3.1 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 2.1/41.0 MB 3.4 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 2.6/41.0 MB 3.0 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 3.1/41.0 MB 2.9 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 3.7/41.0 MB 3.1 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 4.5/41.0 MB 3.0 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 5.0/41.0 MB 3.0 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 6.0/41.0 MB 3.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 6.6/41.0 MB 3.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 7.1/41.0 MB 3.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 7.9/41.0 MB 3.1 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 8.7/41.0 MB 3.2 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 9.4/41.0 MB 3.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 10.5/41.0 MB 3.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 10.7/41.0 MB 3.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 11.0/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 11.8/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.3/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.6/41.0 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.8/41.0 MB 2.9 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.1/41.0 MB 2.9 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.4/41.0 MB 2.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.6/41.0 MB 2.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.9/41.0 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 14.2/41.0 MB 2.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 14.4/41.0 MB 2.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 14.7/41.0 MB 2.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 14.9/41.0 MB 2.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 15.2/41.0 MB 2.4 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 15.5/41.0 MB 2.4 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 15.7/41.0 MB 2.3 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 16.0/41.0 MB 2.3 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 16.3/41.0 MB 2.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 16.5/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 16.8/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 17.0/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 17.6/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 17.8/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 18.1/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 18.4/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 18.6/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 18.9/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 19.1/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 19.7/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 19.9/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 20.4/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 20.7/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 21.0/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 21.5/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 21.8/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 22.3/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 22.5/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 23.1/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 23.3/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 23.6/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 23.9/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 24.1/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 24.4/41.0 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 24.6/41.0 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 25.2/41.0 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 25.7/41.0 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 26.2/41.0 MB 1.9 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 26.7/41.0 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 27.3/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 27.5/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 28.0/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 28.6/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 28.8/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 29.4/41.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 30.1/41.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 30.7/41.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 31.2/41.0 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 31.7/41.0 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 32.2/41.0 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 33.0/41.0 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 33.8/41.0 MB 2.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 34.6/41.0 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 34.9/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.4/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 36.2/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 36.7/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 37.2/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.5/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.5/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.0/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.0/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.8/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 39.1/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.8/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.6/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484af71b-4a69-4d0e-bb2d-d2a8b3d29ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 GPU Önerici'ye hoş geldin! Çıkmak için 'exit' yaz.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  bana ofiste render yapabileceğim yüksek performansta bir gpu lazım ne önerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Açıklamanızdan tam anlam çıkarılamadı. Lütfen örnek vererek detaylandırın (örn. 'orta performansta render yapacağım').\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yüksek performansta render yapmam gerekiyor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: GeForce RTX 3080\n",
      "🔹 VRAM: 10 GB\n",
      "🔹 Performans: Yüksek\n",
      "🔹 Kullanım Alanı: Oyun, Rendering\n",
      "🔹 CUDA Çekirdekleri: 8704.0\n",
      "🔹 Bant Genişliği: 760.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  düşük performans ofis için ne önerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: M3\n",
      "🔹 VRAM: 24 GB\n",
      "🔹 Performans: Düşük\n",
      "🔹 Kullanım Alanı: Ofis, Günlük Kullanım\n",
      "🔹 CUDA Çekirdekleri: 0.0\n",
      "🔹 Bant Genişliği: 0.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  VRAM 32 gb ve üstü olmasını istesem \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Daha net bir öneri için lütfen GPU’dan beklentinizi (örneğin 'orta performanslı', 'yüksek performanslı') belirtin.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  vramin 32 gb ve üsütnde olmasını istesem ayrıca cuda da olmalı ne önerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Daha net bir öneri için lütfen GPU’dan beklentinizi (örneğin 'orta performanslı', 'yüksek performanslı') belirtin.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performans oyun \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: GeForce GTX 1080 Ti\n",
      "🔹 VRAM: 11 GB\n",
      "🔹 Performans: Orta\n",
      "🔹 Kullanım Alanı: Oyun\n",
      "🔹 CUDA Çekirdekleri: 3584.0\n",
      "🔹 Bant Genişliği: 484.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yüksek performans aı\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: GeForce RTX 4070\n",
      "🔹 VRAM: 12 GB\n",
      "🔹 Performans: Yüksek\n",
      "🔹 Kullanım Alanı: Oyun\n",
      "🔹 CUDA Çekirdekleri: 5888.0\n",
      "🔹 Bant Genişliği: 504.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yüksek performans yapay zeka model eğitimi \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: GeForce RTX 4070\n",
      "🔹 VRAM: 12 GB\n",
      "🔹 Performans: Yüksek\n",
      "🔹 Kullanım Alanı: Oyun\n",
      "🔹 CUDA Çekirdekleri: 5888.0\n",
      "🔹 Bant Genişliği: 504.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  çok yüksek performans veri madenciliği için ne önerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: H100 80GB HBM3\n",
      "🔹 VRAM: 80 GB\n",
      "🔹 Performans: Çok Yüksek\n",
      "🔹 Kullanım Alanı: Ofis, Günlük Kullanım\n",
      "🔹 CUDA Çekirdekleri: 16896.0\n",
      "🔹 Bant Genişliği: 3000.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Görüşmek üzere!\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# API Anahtarını ayarla\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n",
    "\n",
    "# CSV'yi oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\Masaüstü\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# Performansları normalize et\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df[\"Kullanım Alanları\"].astype(str) + \" \" + df[\"Performans\"].astype(str)\n",
    "\n",
    "# Embedding modeli\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# IO.net agent\n",
    "agent = Agent(\n",
    "    name=\"GPU Asistanı\",\n",
    "    instructions=\"Kullanıcıdan daha fazla detay almak gerekirse kibarca iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# GPU önerme fonksiyonu\n",
    "def gpu_öner(prompt, threshold=0.5):\n",
    "    prompt = prompt.lower()\n",
    "    performans_seviyesi = None\n",
    "\n",
    "    # Performans seviyesi arama\n",
    "    if \"çok yüksek\" in prompt:\n",
    "        performans_seviyesi = \"çok yüksek\"\n",
    "    elif \"yüksek\" in prompt:\n",
    "        performans_seviyesi = \"yüksek\"\n",
    "    elif \"orta\" in prompt:\n",
    "        performans_seviyesi = \"orta\"\n",
    "    elif \"düşük\" in prompt:\n",
    "        performans_seviyesi = \"düşük\"\n",
    "\n",
    "    # Eğer performans seviyesi belirtilmemişse uyarı ver\n",
    "    if performans_seviyesi is None:\n",
    "        workflow = Workflow(text=prompt, client_mode=False)\n",
    "        workflow.custom(\n",
    "            name=\"detay_iste\",\n",
    "            objective=\"Kullanıcıdan daha fazla detay iste\",\n",
    "            instructions=f\"Kullanıcı şunu yazdı: '{prompt}', ama performans seviyesi eksik. Lütfen tekrar iste.\",\n",
    "            agents=[agent]\n",
    "        )\n",
    "        return \"🤔 Daha net bir öneri için lütfen GPU’dan beklentinizi (örneğin 'orta performanslı', 'yüksek performanslı') belirtin.\"\n",
    "\n",
    "    # Performansa göre filtrele\n",
    "    secilen_df = df[df[\"Performans\"] == performans_seviyesi]\n",
    "    if secilen_df.empty:\n",
    "        return \"🧐 Bu seviyeye uygun GPU bulunamadı. Lütfen kriterlerinizi gözden geçirin.\"\n",
    "\n",
    "    # Embedding & benzerlik hesabı\n",
    "    prompt_embed = embed_model.encode([prompt])\n",
    "    secilen_embeddings = embed_model.encode(secilen_df[\"etiket_birlesik\"].tolist())\n",
    "    skorlar = cosine_similarity(prompt_embed, secilen_embeddings)[0]\n",
    "\n",
    "    max_skor = np.max(skorlar)\n",
    "    if max_skor < threshold:\n",
    "        return \"🤔 Açıklamanızdan tam anlam çıkarılamadı. Lütfen örnek vererek detaylandırın (örn. 'orta performansta render yapacağım').\"\n",
    "\n",
    "    index = np.argmax(skorlar)\n",
    "    secilen_gpu = secilen_df.iloc[index]\n",
    "\n",
    "    return f\"\"\"\n",
    "🎯 Sizin için en uygun GPU:\n",
    "\n",
    "🔹 Model: {secilen_gpu['GPU Modeli']}\n",
    "🔹 VRAM: {secilen_gpu['VRAM (GB)']} GB\n",
    "🔹 Performans: {secilen_gpu['Performans'].title()}\n",
    "🔹 Kullanım Alanı: {secilen_gpu['Kullanım Alanları']}\n",
    "🔹 CUDA Çekirdekleri: {secilen_gpu['CUDA Çekirdekleri']}\n",
    "🔹 Bant Genişliği: {secilen_gpu['Bellek Bant Genişliği (GB/s)']} GB/s\n",
    "\"\"\".strip()\n",
    "\n",
    "# Sohbet\n",
    "def sohbet_giris(prompt, history):\n",
    "    cevap = gpu_öner(prompt)\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    history.append({\"role\": \"assistant\", \"content\": cevap})\n",
    "    return cevap, history\n",
    "\n",
    "# Ana döngü\n",
    "conversation_history = []\n",
    "\n",
    "print(\"💡 GPU Önerici'ye hoş geldin! Çıkmak için 'exit' yaz.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Sen: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"👋 Görüşmek üzere!\")\n",
    "        break\n",
    "\n",
    "    yanit, conversation_history = sohbet_giris(user_input, conversation_history)\n",
    "    print(f\"Bot: {yanit}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e961a70-0a49-4e48-9fd4-13ac711e0ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 GPU Önerici'ye hoş geldin! Çıkmak için 'exit' yaz.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performansta ofis için ne önerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Açıklamanızdan tam anlam çıkarılamadı. Lütfen örnek vererek detaylandırın (örn. 'orta performansta render yapacağım').\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performansta günlük kullanım \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Sizin için en uygun GPU:\n",
      "\n",
      "🔹 Model: GeForce RTX 4060\n",
      "🔹 VRAM: 8 GB\n",
      "🔹 Performans: Orta\n",
      "🔹 Kullanım Alanı: Oyun\n",
      "🔹 CUDA Çekirdekleri: 3072.0\n",
      "🔹 Bant Genişliği: 272.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Görüşmek üzere!\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# API Anahtarı\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-...\"\n",
    "\n",
    "# CSV dosyasını oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\Masaüstü\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# Veriyi temizle ve birleştir\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df[\"Kullanım Alanları\"].astype(str) + \" \" + df[\"Performans\"].astype(str)\n",
    "\n",
    "# Embedding modeli\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Agent tanımı\n",
    "agent = Agent(\n",
    "    name=\"GPU Asistanı\",\n",
    "    instructions=\"Kullanıcıdan daha fazla detay almak gerekirse kibarca iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# GPU önerme fonksiyonu\n",
    "def gpu_öner(prompt, threshold=0.5):\n",
    "    prompt = prompt.lower()\n",
    "    performans_seviyesi = None\n",
    "\n",
    "    if \"çok yüksek\" in prompt:\n",
    "        performans_seviyesi = \"çok yüksek\"\n",
    "    elif \"yüksek\" in prompt:\n",
    "        performans_seviyesi = \"yüksek\"\n",
    "    elif \"orta\" in prompt:\n",
    "        performans_seviyesi = \"orta\"\n",
    "    elif \"düşük\" in prompt:\n",
    "        performans_seviyesi = \"düşük\"\n",
    "\n",
    "    if performans_seviyesi is None:\n",
    "        workflow = Workflow(text=prompt, client_mode=False)\n",
    "        workflow.custom(\n",
    "            name=\"detay_iste\",\n",
    "            objective=\"Kullanıcıdan daha fazla detay iste\",\n",
    "            instructions=f\"Kullanıcı şunu yazdı: '{prompt}', ama performans seviyesi eksik. Lütfen tekrar iste.\",\n",
    "            agents=[agent]\n",
    "        )\n",
    "        return \"🤔 Daha net bir öneri için lütfen GPU’dan beklentinizi (örneğin 'orta performanslı', 'yüksek performanslı') belirtin.\"\n",
    "\n",
    "    secilen_df = df[df[\"Performans\"] == performans_seviyesi].copy()\n",
    "    if secilen_df.empty:\n",
    "        return \"🧐 Bu seviyeye uygun GPU bulunamadı. Lütfen kriterlerinizi gözden geçirin.\"\n",
    "\n",
    "    # Kullanıcı prompt'unun embedding'i\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "\n",
    "    # Sayısal değerleri normalize et (0-1 arası)\n",
    "    def normalize(col):\n",
    "        return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "    secilen_df[\"norm_vram\"] = normalize(secilen_df[\"VRAM (GB)\"])\n",
    "    secilen_df[\"norm_band\"] = normalize(secilen_df[\"Bellek Bant Genişliği (GB/s)\"])\n",
    "    secilen_df[\"norm_cuda\"] = normalize(secilen_df[\"CUDA Çekirdekleri\"])\n",
    "\n",
    "    # Her GPU için embedding + sayısal vektörü birleştir\n",
    "    gpu_embeddings = []\n",
    "    for idx, row in secilen_df.iterrows():\n",
    "        text_embed = embed_model.encode([row[\"etiket_birlesik\"]])[0]\n",
    "        numbers = np.array([row[\"norm_vram\"], row[\"norm_band\"], row[\"norm_cuda\"]])\n",
    "        combined = np.concatenate([text_embed, numbers])\n",
    "        gpu_embeddings.append(combined)\n",
    "\n",
    "    # Aynı şekilde prompt vektörüne sıfırları ekleyerek eşleştir (3 sayısal değer)\n",
    "    prompt_extended = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    # Kosinüs benzerliği\n",
    "    similarity_scores = cosine_similarity([prompt_extended], gpu_embeddings)[0]\n",
    "    max_score = np.max(similarity_scores)\n",
    "\n",
    "    if max_score < threshold:\n",
    "        return \"🤔 Açıklamanızdan tam anlam çıkarılamadı. Lütfen örnek vererek detaylandırın (örn. 'orta performansta render yapacağım').\"\n",
    "\n",
    "    index = np.argmax(similarity_scores)\n",
    "    secilen_gpu = secilen_df.iloc[index]\n",
    "\n",
    "    return f\"\"\"\n",
    "🎯 Sizin için en uygun GPU:\n",
    "\n",
    "🔹 Model: {secilen_gpu['GPU Modeli']}\n",
    "🔹 VRAM: {secilen_gpu['VRAM (GB)']} GB\n",
    "🔹 Performans: {secilen_gpu['Performans'].title()}\n",
    "🔹 Kullanım Alanı: {secilen_gpu['Kullanım Alanları']}\n",
    "🔹 CUDA Çekirdekleri: {secilen_gpu['CUDA Çekirdekleri']}\n",
    "🔹 Bant Genişliği: {secilen_gpu['Bellek Bant Genişliği (GB/s)']} GB/s\n",
    "\"\"\".strip()\n",
    "\n",
    "# Sohbet fonksiyonu\n",
    "def sohbet_giris(prompt, history):\n",
    "    yanit = gpu_öner(prompt)\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    history.append({\"role\": \"assistant\", \"content\": yanit})\n",
    "    return yanit, history\n",
    "\n",
    "# Ana döngü\n",
    "conversation_history = []\n",
    "\n",
    "print(\"💡 GPU Önerici'ye hoş geldin! Çıkmak için 'exit' yaz.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Sen: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"👋 Görüşmek üzere!\")\n",
    "        break\n",
    "\n",
    "    yanit, conversation_history = sohbet_giris(user_input, conversation_history)\n",
    "    print(f\"Bot: {yanit}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12bdb71d-07cd-4c09-8c8c-a47affc99dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Asistanı hazır! 'exit' diyene kadar devam.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yapay zeka eğitimi için ortalama performans \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSen: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m u\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mgpu_oner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m conv\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:u})\n\u001b[0;32m    130\u001b[0m conv\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:res})\n",
      "Cell \u001b[1;32mIn[12], line 107\u001b[0m, in \u001b[0;36mgpu_oner\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m    104\u001b[0m prompt_embed \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mencode([prompt])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m prompt_ext \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([prompt_embed, np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m)])\n\u001b[1;32m--> 107\u001b[0m sims \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_ext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    108\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(sims)\n\u001b[0;32m    109\u001b[0m gpu \u001b[38;5;241m=\u001b[39m df_filt\u001b[38;5;241m.\u001b[39miloc[idx]\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:209\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[1;32m--> 209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOi...\"  \n",
    "\n",
    "# CSV verisini oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\Masaüstü\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# Performansı ve kullanım alanını birleştirerek metin oluştur\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df.apply(\n",
    "    lambda r: f\"{r['GPU Modeli']} {r['Kullanım Alanları']} {r['Performans']} {r['VRAM (GB)']}GB {r['CUDA Çekirdekleri']} çekirdek {r['Bellek Bant Genişliği (GB/s)']}GB/s\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"GPU Önerici Agent\",\n",
    "    instructions=\"Kullanıcının ihtiyaçlarına göre uygun GPU öner. Gerekirse ek bilgi iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "\n",
    "def sayisal_aralik_belirle(prompt, kriter):\n",
    "    if kriter == \"vram\":\n",
    "        m = re.search(r'vram\\s*(\\d+)\\s*gb', prompt)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    if kriter == \"cuda\":\n",
    "        m = re.search(r'cuda\\s*\\d+\\s*çekirdek', prompt)\n",
    "        if m:\n",
    "            return int(re.search(r'(\\d+)', m.group()).group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def performans_belirle(prompt):\n",
    "    pl = prompt.lower()\n",
    "    if \"çok yüksek\" in pl:\n",
    "        return \"çok yüksek\"\n",
    "    if \"yüksek\" in pl:\n",
    "        return \"yüksek\"\n",
    "    if \"orta\" in pl:\n",
    "        return \"orta\"\n",
    "    if \"düşük\" in pl:\n",
    "        return \"düşük\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def kullanım_alani_belirle(prompt):\n",
    "    alanlar = [\"oyun\", \"render\", \"ofis\", \"ai\", \"yapay zeka\"]\n",
    "    for a in alanlar:\n",
    "        if a in prompt.lower():\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# GPU öneri fonksiyonu\n",
    "def gpu_oner(prompt):\n",
    "    p = prompt.lower()\n",
    "    # Filtre kriterleri\n",
    "    min_vram = sayisal_aralik_belirle(p, \"vram\") or 0\n",
    "    min_cuda = sayisal_aralik_belirle(p, \"cuda\") or 0\n",
    "    perf = performans_belirle(p)\n",
    "    alan = kullanım_alani_belirle(p)\n",
    "\n",
    "    df_filt = df.copy()\n",
    "    if min_vram:\n",
    "        df_filt = df_filt[df_filt[\"VRAM (GB)\"] >= min_vram]\n",
    "    if min_cuda:\n",
    "        df_filt = df_filt[df_filt[\"CUDA Çekirdekleri\"] >= min_cuda]\n",
    "    if perf:\n",
    "        df_filt = df_filt[df_filt[\"Performans\"] == perf]\n",
    "    if alan:\n",
    "        df_filt = df_filt[df_filt[\"Kullanım Alanları\"].str.lower().str.contains(alan)]\n",
    "\n",
    "    if df_filt.empty:\n",
    "        return \"🤔 Bu kriterlere uygun GPU bulunamadı. Lütfen daha detaylı bilgi verin.\"\n",
    "\n",
    "    # Normalizasyon\n",
    "    df_filt = df_filt.copy()\n",
    "    df_filt['n_vram'] = normalize(df_filt['VRAM (GB)'])\n",
    "    df_filt['n_cuda'] = normalize(df_filt['CUDA Çekirdekleri'])\n",
    "    df_filt['n_band'] = normalize(df_filt['Bellek Bant Genişliği (GB/s)'])\n",
    "\n",
    "    # Embedding + sayısal vektör\n",
    "    text_embeds = embed_model.encode(df_filt['etiket_birlesik'].tolist())\n",
    "    nums = df_filt[['n_vram','n_cuda','n_band']].values\n",
    "    combined = np.hstack([text_embeds, nums])\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "    prompt_ext = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    sims = cosine_similarity([prompt_ext], combined)[0]\n",
    "    idx = np.argmax(sims)\n",
    "    gpu = df_filt.iloc[idx]\n",
    "\n",
    "    # Agent ile workflow kaydı\n",
    "    wf = Workflow(text=prompt, client_mode=False)\n",
    "    wf.custom(\n",
    "        name=\"gpu_oner_task\",\n",
    "        objective=f\"Prompt: {prompt} - Önerilen GPU: {gpu['GPU Modeli']}\",\n",
    "        instructions=f\"Kullanıcı isteği: {prompt} | Önerilen: {gpu['GPU Modeli']}\",\n",
    "        agents=[agent]\n",
    "    )\n",
    "\n",
    "    return f\"🎯 En uygun GPU: {gpu['GPU Modeli']} (VRAM: {gpu['VRAM (GB)']}GB, CUDA: {gpu['CUDA Çekirdekleri']}, Performans: {gpu['Performans']})\"\n",
    "\n",
    "# Sohbet döngüsü\n",
    "conv = []\n",
    "print(\"GPU Asistanı hazır! 'exit' diyene kadar devam.\")\n",
    "while True:\n",
    "    u = input(\"Sen: \")\n",
    "    if u.lower()=='exit': break\n",
    "    res = gpu_oner(u)\n",
    "    conv.append({'role':'user','content':u})\n",
    "    conv.append({'role':'assistant','content':res})\n",
    "    print(\"Bot:\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4d4599-a5f4-4415-8fd5-ff1324a41903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Asistanı hazır! 'exit' diyene kadar devam.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  ortalama performansta render için gpu ya ihtiyacım var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Bu kriterlere uygun GPU bulunamadı. Lütfen daha detaylı bilgi verin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performans render\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Bu kriterlere uygun GPU bulunamadı. Lütfen daha detaylı bilgi verin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yüksek performans render \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 En uygun GPU: GeForce RTX 4070 Ti (VRAM: 12GB, CUDA: 7680.0, Performans: yüksek)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  ortalama performans günlük kullanım \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 En uygun GPU: RTX 4000 (VRAM: 8GB, CUDA: 2304.0, Performans: orta)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  vram 10 gb ve üstü ortalama ofis kullanımı ü\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🤔 Bu kriterlere uygun GPU bulunamadı. Lütfen daha detaylı bilgi verin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performasn günlük kullanım \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 En uygun GPU: RTX 4000 (VRAM: 8GB, CUDA: 2304.0, Performans: orta)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  düşük performans 10 gb ve üstü vram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 En uygun GPU: M4 (VRAM: 16GB, CUDA: 0.0, Performans: düşük)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 En uygun GPU: RTX A5000 (VRAM: 24GB, CUDA: 8192.0, Performans: yüksek)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# API Anahtarını ayarla\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOi...\"  # Senin gerçek IO.net API anahtarın\n",
    "\n",
    "# CSV verisini oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\Masaüstü\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# Performansı ve kullanım alanını birleştirerek metin oluştur\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df.apply(\n",
    "    lambda r: f\"{r['GPU Modeli']} {r['Kullanım Alanları']} {r['Performans']} {r['VRAM (GB)']}GB {r['CUDA Çekirdekleri']} çekirdek {r['Bellek Bant Genişliği (GB/s)']}GB/s\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Embedding modeli yükle\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# IO.net Agent oluştur\n",
    "agent = Agent(\n",
    "    name=\"GPU Önerici Agent\",\n",
    "    instructions=\"Kullanıcının ihtiyaçlarına göre uygun GPU öner. Gerekirse ek bilgi iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# Yardımcı fonksiyonlar\n",
    "\n",
    "def normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "\n",
    "def sayisal_aralik_belirle(prompt, kriter):\n",
    "    if kriter == \"vram\":\n",
    "        m = re.search(r'vram\\s*(\\d+)\\s*gb', prompt)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    if kriter == \"cuda\":\n",
    "        m = re.search(r'cuda\\s*\\d+\\s*çekirdek', prompt)\n",
    "        if m:\n",
    "            return int(re.search(r'(\\d+)', m.group()).group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def performans_belirle(prompt):\n",
    "    pl = prompt.lower()\n",
    "    if \"çok yüksek\" in pl:\n",
    "        return \"çok yüksek\"\n",
    "    if \"yüksek\" in pl:\n",
    "        return \"yüksek\"\n",
    "    if \"orta\" in pl:\n",
    "        return \"orta\"\n",
    "    if \"düşük\" in pl:\n",
    "        return \"düşük\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def kullanım_alani_belirle(prompt):\n",
    "    alanlar = [\"oyun\", \"render\", \"ofis\", \"ai\", \"yapay zeka\"]\n",
    "    for a in alanlar:\n",
    "        if a in prompt.lower():\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# GPU öneri fonksiyonu\n",
    "def gpu_oner(prompt):\n",
    "    p = prompt.lower()\n",
    "    # Filtre kriterleri\n",
    "    min_vram = sayisal_aralik_belirle(p, \"vram\") or 0\n",
    "    min_cuda = sayisal_aralik_belirle(p, \"cuda\") or 0\n",
    "    perf = performans_belirle(p)\n",
    "    alan = kullanım_alani_belirle(p)\n",
    "\n",
    "    df_filt = df.copy()\n",
    "    if min_vram:\n",
    "        df_filt = df_filt[df_filt[\"VRAM (GB)\"] >= min_vram]\n",
    "    if min_cuda:\n",
    "        df_filt = df_filt[df_filt[\"CUDA Çekirdekleri\"] >= min_cuda]\n",
    "    if perf:\n",
    "        df_filt = df_filt[df_filt[\"Performans\"] == perf]\n",
    "    if alan:\n",
    "        df_filt = df_filt[df_filt[\"Kullanım Alanları\"].str.lower().str.contains(alan)]\n",
    "\n",
    "    if df_filt.empty:\n",
    "        return \"🤔 Bu kriterlere uygun GPU bulunamadı. Lütfen daha detaylı bilgi verin.\"\n",
    "\n",
    "    # Normalize sayısal\n",
    "    df_filt = df_filt.copy()\n",
    "    df_filt['n_vram'] = normalize(df_filt['VRAM (GB)'])\n",
    "    df_filt['n_cuda'] = normalize(df_filt['CUDA Çekirdekleri'])\n",
    "    df_filt['n_band'] = normalize(df_filt['Bellek Bant Genişliği (GB/s)'])\n",
    "\n",
    "    # Embedding + sayısal vektör\n",
    "    text_embeds = embed_model.encode(df_filt['etiket_birlesik'].tolist())\n",
    "    nums = df_filt[['n_vram','n_cuda','n_band']].values\n",
    "    combined = np.hstack([text_embeds, nums])\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "    prompt_ext = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    sims = cosine_similarity([prompt_ext], combined)[0]\n",
    "    idx = np.argmax(sims)\n",
    "    gpu = df_filt.iloc[idx]\n",
    "\n",
    "    # Agent ile workflow kaydı\n",
    "    wf = Workflow(text=prompt, client_mode=False)\n",
    "    wf.custom(\n",
    "        name=\"gpu_oner_task\",\n",
    "        objective=f\"Prompt: {prompt} - Önerilen GPU: {gpu['GPU Modeli']}\",\n",
    "        instructions=f\"Kullanıcı isteği: {prompt} | Önerilen: {gpu['GPU Modeli']}\",\n",
    "        agents=[agent]\n",
    "    )\n",
    "\n",
    "    return f\"🎯 En uygun GPU: {gpu['GPU Modeli']} (VRAM: {gpu['VRAM (GB)']}GB, CUDA: {gpu['CUDA Çekirdekleri']}, Performans: {gpu['Performans']})\"\n",
    "\n",
    "# Sohbet döngüsü\n",
    "conv = []\n",
    "print(\"GPU Asistanı hazır! 'exit' diyene kadar devam.\")\n",
    "while True:\n",
    "    u = input(\"Sen: \")\n",
    "    if u.lower()=='exit': break\n",
    "    res = gpu_oner(u)\n",
    "    conv.append({'role':'user','content':u})\n",
    "    conv.append({'role':'assistant','content':res})\n",
    "    print(\"Bot:\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22845a3e-65cf-4bd3-a411-fff76cd91df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Asistanı hazır! 'exit' diyene kadar devam.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  profesyonel uygulamalar için çok yüksek performansa ihtiyacım var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Önerilen GPU'lar:\n",
      "RTX A6000 (VRAM: 48GB, CUDA: 10752.0, Performans: çok yüksek)\n",
      "GeForce RTX 3090 Ti (VRAM: 24GB, CUDA: 10752.0, Performans: çok yüksek)\n",
      "A100-PCIE-40GB (VRAM: 40GB, CUDA: 6912.0, Performans: çok yüksek)\n",
      "GeForce RTX 3090 (VRAM: 24GB, CUDA: 10496.0, Performans: çok yüksek)\n",
      "GeForce RTX 3080 Ti (VRAM: 12GB, CUDA: 10240.0, Performans: çok yüksek)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  oyun için orta performansta hangi gpuları önerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 🎯 Önerilen GPU'lar:\n",
      "GeForce RTX 3060 (VRAM: 12GB, CUDA: 3584.0, Performans: orta)\n",
      "GeForce RTX 4060 (VRAM: 8GB, CUDA: 3072.0, Performans: orta)\n",
      "GeForce RTX 4060 Ti (VRAM: 8GB, CUDA: 4352.0, Performans: orta)\n",
      "GeForce GTX 1080 Ti (VRAM: 11GB, CUDA: 3584.0, Performans: orta)\n",
      "GeForce RTX 3060 Ti (VRAM: 8GB, CUDA: 4864.0, Performans: orta)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# API Anahtarını ayarla\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOi...\"  # Gerçek IO.net API anahtarını buraya gir\n",
    "\n",
    "# CSV verisini oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\Masaüstü\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# Performansı ve kullanım alanını birleştirerek metin oluştur\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df.apply(\n",
    "    lambda r: f\"{r['GPU Modeli']} {r['Kullanım Alanları']} {r['Performans']} {r['VRAM (GB)']}GB {r['CUDA Çekirdekleri']} çekirdek {r['Bellek Bant Genişliği (GB/s)']}GB/s\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# NaN değerleri doldur\n",
    "df.ffill(inplace=True)\n",
    "\n",
    "# Embedding modeli yükle\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# IO.net Agent oluştur\n",
    "agent = Agent(\n",
    "    name=\"GPU Önerici Agent\",\n",
    "    instructions=\"Kullanıcının ihtiyaçlarına göre uygun GPU öner. Gerekirse ek bilgi iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# Yardımcı fonksiyonlar\n",
    "def normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "def sayisal_aralik_belirle(prompt, kriter):\n",
    "    if kriter == \"vram\":\n",
    "        m = re.search(r'vram\\s*(\\d+)\\s*gb', prompt)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    if kriter == \"cuda\":\n",
    "        m = re.search(r'cuda\\s*\\d+\\s*çekirdek', prompt)\n",
    "        if m:\n",
    "            return int(re.search(r'(\\d+)', m.group()).group(1))\n",
    "    return None\n",
    "\n",
    "def performans_belirle(prompt):\n",
    "    pl = prompt.lower()\n",
    "    if \"çok yüksek\" in pl:\n",
    "        return \"çok yüksek\"\n",
    "    if \"yüksek\" in pl:\n",
    "        return \"yüksek\"\n",
    "    if \"orta\" in pl:\n",
    "        return \"orta\"\n",
    "    if \"düşük\" in pl:\n",
    "        return \"düşük\"\n",
    "    return None\n",
    "\n",
    "def kullanım_alani_belirle(prompt):\n",
    "    alanlar = [\"oyun\", \"render\", \"ofis\", \"ai\", \"yapay zeka\"]\n",
    "    for a in alanlar:\n",
    "        if a in prompt.lower():\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# GPU öneri fonksiyonu\n",
    "def gpu_oner(prompt, max_results=5):\n",
    "    p = prompt.lower()\n",
    "    min_vram = sayisal_aralik_belirle(p, \"vram\") or 0\n",
    "    min_cuda = sayisal_aralik_belirle(p, \"cuda\") or 0\n",
    "    perf = performans_belirle(p)\n",
    "    alan = kullanım_alani_belirle(p)\n",
    "\n",
    "    df_filt = df.copy()\n",
    "    if min_vram:\n",
    "        df_filt = df_filt[df_filt[\"VRAM (GB)\"] >= min_vram]\n",
    "    if min_cuda:\n",
    "        df_filt = df_filt[df_filt[\"CUDA Çekirdekleri\"] >= min_cuda]\n",
    "    if perf:\n",
    "        df_filt = df_filt[df_filt[\"Performans\"] == perf]\n",
    "    if alan:\n",
    "        df_filt = df_filt[df_filt[\"Kullanım Alanları\"].str.lower().str.contains(alan)]\n",
    "\n",
    "    if df_filt.empty:\n",
    "        return \"🤔 Bu kriterlere uygun GPU bulunamadı. Lütfen daha detaylı bilgi verin.\"\n",
    "\n",
    "    df_filt.ffill(inplace=True)\n",
    "\n",
    "    df_filt['n_vram'] = normalize(df_filt['VRAM (GB)'])\n",
    "    df_filt['n_cuda'] = normalize(df_filt['CUDA Çekirdekleri'])\n",
    "    df_filt['n_band'] = normalize(df_filt['Bellek Bant Genişliği (GB/s)'])\n",
    "\n",
    "    text_embeds = embed_model.encode(df_filt['etiket_birlesik'].tolist())\n",
    "    nums = df_filt[['n_vram','n_cuda','n_band']].values\n",
    "    combined = np.hstack([text_embeds, nums])\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "    prompt_ext = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    sims = cosine_similarity([prompt_ext], combined)[0]\n",
    "    idx = np.argsort(sims)[-max_results:][::-1]\n",
    "\n",
    "    gpu_list = []\n",
    "    for i in idx:\n",
    "        gpu = df_filt.iloc[i]\n",
    "        gpu_list.append(f\"{gpu['GPU Modeli']} (VRAM: {gpu['VRAM (GB)']}GB, CUDA: {gpu['CUDA Çekirdekleri']}, Performans: {gpu['Performans']})\")\n",
    "\n",
    "    # Agent ile kayıt\n",
    "    wf = Workflow(text=prompt, client_mode=False)\n",
    "    wf.custom(\n",
    "        name=\"gpu_oner_task\",\n",
    "        objective=f\"Prompt: {prompt} - Önerilen GPU'lar:\\n\" + \"\\n\".join(gpu_list),\n",
    "        instructions=f\"Kullanıcı isteği: {prompt} | Önerilenler:\\n\" + \"\\n\".join(gpu_list),\n",
    "        agents=[agent]\n",
    "    )\n",
    "\n",
    "    return f\"🎯 Önerilen GPU'lar:\\n\" + \"\\n\".join(gpu_list)\n",
    "\n",
    "# Sohbet başlat\n",
    "conv = []\n",
    "print(\"GPU Asistanı hazır! 'exit' diyene kadar devam.\")\n",
    "while True:\n",
    "    u = input(\"Sen: \")\n",
    "    if u.lower() == 'exit':\n",
    "        break\n",
    "    res = gpu_oner(u)\n",
    "    conv.append({'role': 'user', 'content': u})\n",
    "    conv.append({'role': 'assistant', 'content': res})\n",
    "    print(\"Bot:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb6e64-20d7-4eee-9a9e-542893d65a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
