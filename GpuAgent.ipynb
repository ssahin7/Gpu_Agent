{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec923832-d40d-42ea-8cfd-9de71eddb74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pendulum\n",
      "  Downloading pendulum-3.1.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6 in d:\\python\\lib\\site-packages (from pendulum) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2020.1 in d:\\python\\lib\\site-packages (from pendulum) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.6->pendulum) (1.17.0)\n",
      "Downloading pendulum-3.1.0-cp313-cp313-win_amd64.whl (260 kB)\n",
      "Installing collected packages: pendulum\n",
      "Successfully installed pendulum-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f419017a-fbec-4a8c-b070-4d6865280c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\python\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.2/11.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.5/11.5 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.0/11.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.1/11.5 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.9/11.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.5 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1335898a-5472-4f84-9a6d-0d3bcdcbba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\python\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.1 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.2/11.1 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.1 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/41.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/41.0 MB 3.1 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 2.1/41.0 MB 3.4 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 2.6/41.0 MB 3.0 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 3.1/41.0 MB 2.9 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 3.7/41.0 MB 3.1 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 4.5/41.0 MB 3.0 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 5.0/41.0 MB 3.0 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 6.0/41.0 MB 3.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 6.6/41.0 MB 3.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 7.1/41.0 MB 3.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 7.9/41.0 MB 3.1 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 8.7/41.0 MB 3.2 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 9.4/41.0 MB 3.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 10.5/41.0 MB 3.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 10.7/41.0 MB 3.3 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 11.0/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 11.8/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.3/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.6/41.0 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.8/41.0 MB 2.9 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.1/41.0 MB 2.9 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.4/41.0 MB 2.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.6/41.0 MB 2.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.9/41.0 MB 2.7 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 14.2/41.0 MB 2.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 14.4/41.0 MB 2.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 14.7/41.0 MB 2.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 14.9/41.0 MB 2.5 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 15.2/41.0 MB 2.4 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 15.5/41.0 MB 2.4 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 15.7/41.0 MB 2.3 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 16.0/41.0 MB 2.3 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 16.3/41.0 MB 2.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 16.5/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 16.8/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 17.0/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 17.6/41.0 MB 2.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 17.8/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 18.1/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 18.4/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 18.6/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 18.9/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 19.1/41.0 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 19.7/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 19.9/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 20.4/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 20.7/41.0 MB 2.0 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 21.0/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 21.5/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 21.8/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 22.3/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 22.5/41.0 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 23.1/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 23.3/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 23.6/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 23.9/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 24.1/41.0 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 24.4/41.0 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 24.6/41.0 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 25.2/41.0 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 25.7/41.0 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 26.2/41.0 MB 1.9 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 26.7/41.0 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 27.3/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 27.5/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 28.0/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 28.6/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 28.8/41.0 MB 2.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 29.4/41.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 30.1/41.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 30.7/41.0 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 31.2/41.0 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 31.7/41.0 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 32.2/41.0 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 33.0/41.0 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 33.8/41.0 MB 2.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 34.6/41.0 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 34.9/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.4/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 36.2/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 36.7/41.0 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 37.2/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.5/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.5/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.0/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.0/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.8/41.0 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 39.1/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.8/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.6/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484af71b-4a69-4d0e-bb2d-d2a8b3d29ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ GPU Ã–nerici'ye hoÅŸ geldin! Ã‡Ä±kmak iÃ§in 'exit' yaz.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  bana ofiste render yapabileceÄŸim yÃ¼ksek performansta bir gpu lazÄ±m ne Ã¶nerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” AÃ§Ä±klamanÄ±zdan tam anlam Ã§Ä±karÄ±lamadÄ±. LÃ¼tfen Ã¶rnek vererek detaylandÄ±rÄ±n (Ã¶rn. 'orta performansta render yapacaÄŸÄ±m').\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yÃ¼ksek performansta render yapmam gerekiyor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: GeForce RTX 3080\n",
      "ğŸ”¹ VRAM: 10 GB\n",
      "ğŸ”¹ Performans: YÃ¼ksek\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Oyun, Rendering\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 8704.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 760.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  dÃ¼ÅŸÃ¼k performans ofis iÃ§in ne Ã¶nerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: M3\n",
      "ğŸ”¹ VRAM: 24 GB\n",
      "ğŸ”¹ Performans: DÃ¼ÅŸÃ¼k\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Ofis, GÃ¼nlÃ¼k KullanÄ±m\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 0.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 0.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  VRAM 32 gb ve Ã¼stÃ¼ olmasÄ±nÄ± istesem \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” Daha net bir Ã¶neri iÃ§in lÃ¼tfen GPUâ€™dan beklentinizi (Ã¶rneÄŸin 'orta performanslÄ±', 'yÃ¼ksek performanslÄ±') belirtin.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  vramin 32 gb ve Ã¼sÃ¼tnde olmasÄ±nÄ± istesem ayrÄ±ca cuda da olmalÄ± ne Ã¶nerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” Daha net bir Ã¶neri iÃ§in lÃ¼tfen GPUâ€™dan beklentinizi (Ã¶rneÄŸin 'orta performanslÄ±', 'yÃ¼ksek performanslÄ±') belirtin.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performans oyun \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: GeForce GTX 1080 Ti\n",
      "ğŸ”¹ VRAM: 11 GB\n",
      "ğŸ”¹ Performans: Orta\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Oyun\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 3584.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 484.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yÃ¼ksek performans aÄ±\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: GeForce RTX 4070\n",
      "ğŸ”¹ VRAM: 12 GB\n",
      "ğŸ”¹ Performans: YÃ¼ksek\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Oyun\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 5888.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 504.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yÃ¼ksek performans yapay zeka model eÄŸitimi \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: GeForce RTX 4070\n",
      "ğŸ”¹ VRAM: 12 GB\n",
      "ğŸ”¹ Performans: YÃ¼ksek\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Oyun\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 5888.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 504.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  Ã§ok yÃ¼ksek performans veri madenciliÄŸi iÃ§in ne Ã¶nerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: H100 80GB HBM3\n",
      "ğŸ”¹ VRAM: 80 GB\n",
      "ğŸ”¹ Performans: Ã‡ok YÃ¼ksek\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Ofis, GÃ¼nlÃ¼k KullanÄ±m\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 16896.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 3000.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# API AnahtarÄ±nÄ± ayarla\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n",
    "\n",
    "# CSV'yi oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\MasaÃ¼stÃ¼\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# PerformanslarÄ± normalize et\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df[\"KullanÄ±m AlanlarÄ±\"].astype(str) + \" \" + df[\"Performans\"].astype(str)\n",
    "\n",
    "# Embedding modeli\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# IO.net agent\n",
    "agent = Agent(\n",
    "    name=\"GPU AsistanÄ±\",\n",
    "    instructions=\"KullanÄ±cÄ±dan daha fazla detay almak gerekirse kibarca iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# GPU Ã¶nerme fonksiyonu\n",
    "def gpu_Ã¶ner(prompt, threshold=0.5):\n",
    "    prompt = prompt.lower()\n",
    "    performans_seviyesi = None\n",
    "\n",
    "    # Performans seviyesi arama\n",
    "    if \"Ã§ok yÃ¼ksek\" in prompt:\n",
    "        performans_seviyesi = \"Ã§ok yÃ¼ksek\"\n",
    "    elif \"yÃ¼ksek\" in prompt:\n",
    "        performans_seviyesi = \"yÃ¼ksek\"\n",
    "    elif \"orta\" in prompt:\n",
    "        performans_seviyesi = \"orta\"\n",
    "    elif \"dÃ¼ÅŸÃ¼k\" in prompt:\n",
    "        performans_seviyesi = \"dÃ¼ÅŸÃ¼k\"\n",
    "\n",
    "    # EÄŸer performans seviyesi belirtilmemiÅŸse uyarÄ± ver\n",
    "    if performans_seviyesi is None:\n",
    "        workflow = Workflow(text=prompt, client_mode=False)\n",
    "        workflow.custom(\n",
    "            name=\"detay_iste\",\n",
    "            objective=\"KullanÄ±cÄ±dan daha fazla detay iste\",\n",
    "            instructions=f\"KullanÄ±cÄ± ÅŸunu yazdÄ±: '{prompt}', ama performans seviyesi eksik. LÃ¼tfen tekrar iste.\",\n",
    "            agents=[agent]\n",
    "        )\n",
    "        return \"ğŸ¤” Daha net bir Ã¶neri iÃ§in lÃ¼tfen GPUâ€™dan beklentinizi (Ã¶rneÄŸin 'orta performanslÄ±', 'yÃ¼ksek performanslÄ±') belirtin.\"\n",
    "\n",
    "    # Performansa gÃ¶re filtrele\n",
    "    secilen_df = df[df[\"Performans\"] == performans_seviyesi]\n",
    "    if secilen_df.empty:\n",
    "        return \"ğŸ§ Bu seviyeye uygun GPU bulunamadÄ±. LÃ¼tfen kriterlerinizi gÃ¶zden geÃ§irin.\"\n",
    "\n",
    "    # Embedding & benzerlik hesabÄ±\n",
    "    prompt_embed = embed_model.encode([prompt])\n",
    "    secilen_embeddings = embed_model.encode(secilen_df[\"etiket_birlesik\"].tolist())\n",
    "    skorlar = cosine_similarity(prompt_embed, secilen_embeddings)[0]\n",
    "\n",
    "    max_skor = np.max(skorlar)\n",
    "    if max_skor < threshold:\n",
    "        return \"ğŸ¤” AÃ§Ä±klamanÄ±zdan tam anlam Ã§Ä±karÄ±lamadÄ±. LÃ¼tfen Ã¶rnek vererek detaylandÄ±rÄ±n (Ã¶rn. 'orta performansta render yapacaÄŸÄ±m').\"\n",
    "\n",
    "    index = np.argmax(skorlar)\n",
    "    secilen_gpu = secilen_df.iloc[index]\n",
    "\n",
    "    return f\"\"\"\n",
    "ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
    "\n",
    "ğŸ”¹ Model: {secilen_gpu['GPU Modeli']}\n",
    "ğŸ”¹ VRAM: {secilen_gpu['VRAM (GB)']} GB\n",
    "ğŸ”¹ Performans: {secilen_gpu['Performans'].title()}\n",
    "ğŸ”¹ KullanÄ±m AlanÄ±: {secilen_gpu['KullanÄ±m AlanlarÄ±']}\n",
    "ğŸ”¹ CUDA Ã‡ekirdekleri: {secilen_gpu['CUDA Ã‡ekirdekleri']}\n",
    "ğŸ”¹ Bant GeniÅŸliÄŸi: {secilen_gpu['Bellek Bant GeniÅŸliÄŸi (GB/s)']} GB/s\n",
    "\"\"\".strip()\n",
    "\n",
    "# Sohbet\n",
    "def sohbet_giris(prompt, history):\n",
    "    cevap = gpu_Ã¶ner(prompt)\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    history.append({\"role\": \"assistant\", \"content\": cevap})\n",
    "    return cevap, history\n",
    "\n",
    "# Ana dÃ¶ngÃ¼\n",
    "conversation_history = []\n",
    "\n",
    "print(\"ğŸ’¡ GPU Ã–nerici'ye hoÅŸ geldin! Ã‡Ä±kmak iÃ§in 'exit' yaz.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Sen: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\")\n",
    "        break\n",
    "\n",
    "    yanit, conversation_history = sohbet_giris(user_input, conversation_history)\n",
    "    print(f\"Bot: {yanit}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e961a70-0a49-4e48-9fd4-13ac711e0ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ GPU Ã–nerici'ye hoÅŸ geldin! Ã‡Ä±kmak iÃ§in 'exit' yaz.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performansta ofis iÃ§in ne Ã¶nerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” AÃ§Ä±klamanÄ±zdan tam anlam Ã§Ä±karÄ±lamadÄ±. LÃ¼tfen Ã¶rnek vererek detaylandÄ±rÄ±n (Ã¶rn. 'orta performansta render yapacaÄŸÄ±m').\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performansta gÃ¼nlÃ¼k kullanÄ±m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
      "\n",
      "ğŸ”¹ Model: GeForce RTX 4060\n",
      "ğŸ”¹ VRAM: 8 GB\n",
      "ğŸ”¹ Performans: Orta\n",
      "ğŸ”¹ KullanÄ±m AlanÄ±: Oyun\n",
      "ğŸ”¹ CUDA Ã‡ekirdekleri: 3072.0\n",
      "ğŸ”¹ Bant GeniÅŸliÄŸi: 272.0 GB/s\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# API AnahtarÄ±\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-...\"\n",
    "\n",
    "# CSV dosyasÄ±nÄ± oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\MasaÃ¼stÃ¼\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# Veriyi temizle ve birleÅŸtir\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df[\"KullanÄ±m AlanlarÄ±\"].astype(str) + \" \" + df[\"Performans\"].astype(str)\n",
    "\n",
    "# Embedding modeli\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Agent tanÄ±mÄ±\n",
    "agent = Agent(\n",
    "    name=\"GPU AsistanÄ±\",\n",
    "    instructions=\"KullanÄ±cÄ±dan daha fazla detay almak gerekirse kibarca iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# GPU Ã¶nerme fonksiyonu\n",
    "def gpu_Ã¶ner(prompt, threshold=0.5):\n",
    "    prompt = prompt.lower()\n",
    "    performans_seviyesi = None\n",
    "\n",
    "    if \"Ã§ok yÃ¼ksek\" in prompt:\n",
    "        performans_seviyesi = \"Ã§ok yÃ¼ksek\"\n",
    "    elif \"yÃ¼ksek\" in prompt:\n",
    "        performans_seviyesi = \"yÃ¼ksek\"\n",
    "    elif \"orta\" in prompt:\n",
    "        performans_seviyesi = \"orta\"\n",
    "    elif \"dÃ¼ÅŸÃ¼k\" in prompt:\n",
    "        performans_seviyesi = \"dÃ¼ÅŸÃ¼k\"\n",
    "\n",
    "    if performans_seviyesi is None:\n",
    "        workflow = Workflow(text=prompt, client_mode=False)\n",
    "        workflow.custom(\n",
    "            name=\"detay_iste\",\n",
    "            objective=\"KullanÄ±cÄ±dan daha fazla detay iste\",\n",
    "            instructions=f\"KullanÄ±cÄ± ÅŸunu yazdÄ±: '{prompt}', ama performans seviyesi eksik. LÃ¼tfen tekrar iste.\",\n",
    "            agents=[agent]\n",
    "        )\n",
    "        return \"ğŸ¤” Daha net bir Ã¶neri iÃ§in lÃ¼tfen GPUâ€™dan beklentinizi (Ã¶rneÄŸin 'orta performanslÄ±', 'yÃ¼ksek performanslÄ±') belirtin.\"\n",
    "\n",
    "    secilen_df = df[df[\"Performans\"] == performans_seviyesi].copy()\n",
    "    if secilen_df.empty:\n",
    "        return \"ğŸ§ Bu seviyeye uygun GPU bulunamadÄ±. LÃ¼tfen kriterlerinizi gÃ¶zden geÃ§irin.\"\n",
    "\n",
    "    # KullanÄ±cÄ± prompt'unun embedding'i\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "\n",
    "    # SayÄ±sal deÄŸerleri normalize et (0-1 arasÄ±)\n",
    "    def normalize(col):\n",
    "        return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "    secilen_df[\"norm_vram\"] = normalize(secilen_df[\"VRAM (GB)\"])\n",
    "    secilen_df[\"norm_band\"] = normalize(secilen_df[\"Bellek Bant GeniÅŸliÄŸi (GB/s)\"])\n",
    "    secilen_df[\"norm_cuda\"] = normalize(secilen_df[\"CUDA Ã‡ekirdekleri\"])\n",
    "\n",
    "    # Her GPU iÃ§in embedding + sayÄ±sal vektÃ¶rÃ¼ birleÅŸtir\n",
    "    gpu_embeddings = []\n",
    "    for idx, row in secilen_df.iterrows():\n",
    "        text_embed = embed_model.encode([row[\"etiket_birlesik\"]])[0]\n",
    "        numbers = np.array([row[\"norm_vram\"], row[\"norm_band\"], row[\"norm_cuda\"]])\n",
    "        combined = np.concatenate([text_embed, numbers])\n",
    "        gpu_embeddings.append(combined)\n",
    "\n",
    "    # AynÄ± ÅŸekilde prompt vektÃ¶rÃ¼ne sÄ±fÄ±rlarÄ± ekleyerek eÅŸleÅŸtir (3 sayÄ±sal deÄŸer)\n",
    "    prompt_extended = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    # KosinÃ¼s benzerliÄŸi\n",
    "    similarity_scores = cosine_similarity([prompt_extended], gpu_embeddings)[0]\n",
    "    max_score = np.max(similarity_scores)\n",
    "\n",
    "    if max_score < threshold:\n",
    "        return \"ğŸ¤” AÃ§Ä±klamanÄ±zdan tam anlam Ã§Ä±karÄ±lamadÄ±. LÃ¼tfen Ã¶rnek vererek detaylandÄ±rÄ±n (Ã¶rn. 'orta performansta render yapacaÄŸÄ±m').\"\n",
    "\n",
    "    index = np.argmax(similarity_scores)\n",
    "    secilen_gpu = secilen_df.iloc[index]\n",
    "\n",
    "    return f\"\"\"\n",
    "ğŸ¯ Sizin iÃ§in en uygun GPU:\n",
    "\n",
    "ğŸ”¹ Model: {secilen_gpu['GPU Modeli']}\n",
    "ğŸ”¹ VRAM: {secilen_gpu['VRAM (GB)']} GB\n",
    "ğŸ”¹ Performans: {secilen_gpu['Performans'].title()}\n",
    "ğŸ”¹ KullanÄ±m AlanÄ±: {secilen_gpu['KullanÄ±m AlanlarÄ±']}\n",
    "ğŸ”¹ CUDA Ã‡ekirdekleri: {secilen_gpu['CUDA Ã‡ekirdekleri']}\n",
    "ğŸ”¹ Bant GeniÅŸliÄŸi: {secilen_gpu['Bellek Bant GeniÅŸliÄŸi (GB/s)']} GB/s\n",
    "\"\"\".strip()\n",
    "\n",
    "# Sohbet fonksiyonu\n",
    "def sohbet_giris(prompt, history):\n",
    "    yanit = gpu_Ã¶ner(prompt)\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    history.append({\"role\": \"assistant\", \"content\": yanit})\n",
    "    return yanit, history\n",
    "\n",
    "# Ana dÃ¶ngÃ¼\n",
    "conversation_history = []\n",
    "\n",
    "print(\"ğŸ’¡ GPU Ã–nerici'ye hoÅŸ geldin! Ã‡Ä±kmak iÃ§in 'exit' yaz.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Sen: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\")\n",
    "        break\n",
    "\n",
    "    yanit, conversation_history = sohbet_giris(user_input, conversation_history)\n",
    "    print(f\"Bot: {yanit}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12bdb71d-07cd-4c09-8c8c-a47affc99dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU AsistanÄ± hazÄ±r! 'exit' diyene kadar devam.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yapay zeka eÄŸitimi iÃ§in ortalama performans \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSen: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m u\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mgpu_oner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m conv\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:u})\n\u001b[0;32m    130\u001b[0m conv\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:res})\n",
      "Cell \u001b[1;32mIn[12], line 107\u001b[0m, in \u001b[0;36mgpu_oner\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m    104\u001b[0m prompt_embed \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mencode([prompt])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m prompt_ext \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([prompt_embed, np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m)])\n\u001b[1;32m--> 107\u001b[0m sims \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_ext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    108\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(sims)\n\u001b[0;32m    109\u001b[0m gpu \u001b[38;5;241m=\u001b[39m df_filt\u001b[38;5;241m.\u001b[39miloc[idx]\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:209\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[1;32m--> 209\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOi...\"  \n",
    "\n",
    "# CSV verisini oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\MasaÃ¼stÃ¼\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# PerformansÄ± ve kullanÄ±m alanÄ±nÄ± birleÅŸtirerek metin oluÅŸtur\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df.apply(\n",
    "    lambda r: f\"{r['GPU Modeli']} {r['KullanÄ±m AlanlarÄ±']} {r['Performans']} {r['VRAM (GB)']}GB {r['CUDA Ã‡ekirdekleri']} Ã§ekirdek {r['Bellek Bant GeniÅŸliÄŸi (GB/s)']}GB/s\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"GPU Ã–nerici Agent\",\n",
    "    instructions=\"KullanÄ±cÄ±nÄ±n ihtiyaÃ§larÄ±na gÃ¶re uygun GPU Ã¶ner. Gerekirse ek bilgi iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "\n",
    "def sayisal_aralik_belirle(prompt, kriter):\n",
    "    if kriter == \"vram\":\n",
    "        m = re.search(r'vram\\s*(\\d+)\\s*gb', prompt)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    if kriter == \"cuda\":\n",
    "        m = re.search(r'cuda\\s*\\d+\\s*Ã§ekirdek', prompt)\n",
    "        if m:\n",
    "            return int(re.search(r'(\\d+)', m.group()).group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def performans_belirle(prompt):\n",
    "    pl = prompt.lower()\n",
    "    if \"Ã§ok yÃ¼ksek\" in pl:\n",
    "        return \"Ã§ok yÃ¼ksek\"\n",
    "    if \"yÃ¼ksek\" in pl:\n",
    "        return \"yÃ¼ksek\"\n",
    "    if \"orta\" in pl:\n",
    "        return \"orta\"\n",
    "    if \"dÃ¼ÅŸÃ¼k\" in pl:\n",
    "        return \"dÃ¼ÅŸÃ¼k\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def kullanÄ±m_alani_belirle(prompt):\n",
    "    alanlar = [\"oyun\", \"render\", \"ofis\", \"ai\", \"yapay zeka\"]\n",
    "    for a in alanlar:\n",
    "        if a in prompt.lower():\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# GPU Ã¶neri fonksiyonu\n",
    "def gpu_oner(prompt):\n",
    "    p = prompt.lower()\n",
    "    # Filtre kriterleri\n",
    "    min_vram = sayisal_aralik_belirle(p, \"vram\") or 0\n",
    "    min_cuda = sayisal_aralik_belirle(p, \"cuda\") or 0\n",
    "    perf = performans_belirle(p)\n",
    "    alan = kullanÄ±m_alani_belirle(p)\n",
    "\n",
    "    df_filt = df.copy()\n",
    "    if min_vram:\n",
    "        df_filt = df_filt[df_filt[\"VRAM (GB)\"] >= min_vram]\n",
    "    if min_cuda:\n",
    "        df_filt = df_filt[df_filt[\"CUDA Ã‡ekirdekleri\"] >= min_cuda]\n",
    "    if perf:\n",
    "        df_filt = df_filt[df_filt[\"Performans\"] == perf]\n",
    "    if alan:\n",
    "        df_filt = df_filt[df_filt[\"KullanÄ±m AlanlarÄ±\"].str.lower().str.contains(alan)]\n",
    "\n",
    "    if df_filt.empty:\n",
    "        return \"ğŸ¤” Bu kriterlere uygun GPU bulunamadÄ±. LÃ¼tfen daha detaylÄ± bilgi verin.\"\n",
    "\n",
    "    # Normalizasyon\n",
    "    df_filt = df_filt.copy()\n",
    "    df_filt['n_vram'] = normalize(df_filt['VRAM (GB)'])\n",
    "    df_filt['n_cuda'] = normalize(df_filt['CUDA Ã‡ekirdekleri'])\n",
    "    df_filt['n_band'] = normalize(df_filt['Bellek Bant GeniÅŸliÄŸi (GB/s)'])\n",
    "\n",
    "    # Embedding + sayÄ±sal vektÃ¶r\n",
    "    text_embeds = embed_model.encode(df_filt['etiket_birlesik'].tolist())\n",
    "    nums = df_filt[['n_vram','n_cuda','n_band']].values\n",
    "    combined = np.hstack([text_embeds, nums])\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "    prompt_ext = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    sims = cosine_similarity([prompt_ext], combined)[0]\n",
    "    idx = np.argmax(sims)\n",
    "    gpu = df_filt.iloc[idx]\n",
    "\n",
    "    # Agent ile workflow kaydÄ±\n",
    "    wf = Workflow(text=prompt, client_mode=False)\n",
    "    wf.custom(\n",
    "        name=\"gpu_oner_task\",\n",
    "        objective=f\"Prompt: {prompt} - Ã–nerilen GPU: {gpu['GPU Modeli']}\",\n",
    "        instructions=f\"KullanÄ±cÄ± isteÄŸi: {prompt} | Ã–nerilen: {gpu['GPU Modeli']}\",\n",
    "        agents=[agent]\n",
    "    )\n",
    "\n",
    "    return f\"ğŸ¯ En uygun GPU: {gpu['GPU Modeli']} (VRAM: {gpu['VRAM (GB)']}GB, CUDA: {gpu['CUDA Ã‡ekirdekleri']}, Performans: {gpu['Performans']})\"\n",
    "\n",
    "# Sohbet dÃ¶ngÃ¼sÃ¼\n",
    "conv = []\n",
    "print(\"GPU AsistanÄ± hazÄ±r! 'exit' diyene kadar devam.\")\n",
    "while True:\n",
    "    u = input(\"Sen: \")\n",
    "    if u.lower()=='exit': break\n",
    "    res = gpu_oner(u)\n",
    "    conv.append({'role':'user','content':u})\n",
    "    conv.append({'role':'assistant','content':res})\n",
    "    print(\"Bot:\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4d4599-a5f4-4415-8fd5-ff1324a41903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU AsistanÄ± hazÄ±r! 'exit' diyene kadar devam.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  ortalama performansta render iÃ§in gpu ya ihtiyacÄ±m var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” Bu kriterlere uygun GPU bulunamadÄ±. LÃ¼tfen daha detaylÄ± bilgi verin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performans render\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” Bu kriterlere uygun GPU bulunamadÄ±. LÃ¼tfen daha detaylÄ± bilgi verin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  yÃ¼ksek performans render \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ En uygun GPU: GeForce RTX 4070 Ti (VRAM: 12GB, CUDA: 7680.0, Performans: yÃ¼ksek)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  ortalama performans gÃ¼nlÃ¼k kullanÄ±m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ En uygun GPU: RTX 4000 (VRAM: 8GB, CUDA: 2304.0, Performans: orta)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  vram 10 gb ve Ã¼stÃ¼ ortalama ofis kullanÄ±mÄ± Ã¼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¤” Bu kriterlere uygun GPU bulunamadÄ±. LÃ¼tfen daha detaylÄ± bilgi verin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  orta performasn gÃ¼nlÃ¼k kullanÄ±m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ En uygun GPU: RTX 4000 (VRAM: 8GB, CUDA: 2304.0, Performans: orta)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  dÃ¼ÅŸÃ¼k performans 10 gb ve Ã¼stÃ¼ vram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ En uygun GPU: M4 (VRAM: 16GB, CUDA: 0.0, Performans: dÃ¼ÅŸÃ¼k)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ En uygun GPU: RTX A5000 (VRAM: 24GB, CUDA: 8192.0, Performans: yÃ¼ksek)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# API AnahtarÄ±nÄ± ayarla\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOi...\"  # Senin gerÃ§ek IO.net API anahtarÄ±n\n",
    "\n",
    "# CSV verisini oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\MasaÃ¼stÃ¼\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# PerformansÄ± ve kullanÄ±m alanÄ±nÄ± birleÅŸtirerek metin oluÅŸtur\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df.apply(\n",
    "    lambda r: f\"{r['GPU Modeli']} {r['KullanÄ±m AlanlarÄ±']} {r['Performans']} {r['VRAM (GB)']}GB {r['CUDA Ã‡ekirdekleri']} Ã§ekirdek {r['Bellek Bant GeniÅŸliÄŸi (GB/s)']}GB/s\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Embedding modeli yÃ¼kle\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# IO.net Agent oluÅŸtur\n",
    "agent = Agent(\n",
    "    name=\"GPU Ã–nerici Agent\",\n",
    "    instructions=\"KullanÄ±cÄ±nÄ±n ihtiyaÃ§larÄ±na gÃ¶re uygun GPU Ã¶ner. Gerekirse ek bilgi iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# YardÄ±mcÄ± fonksiyonlar\n",
    "\n",
    "def normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "\n",
    "def sayisal_aralik_belirle(prompt, kriter):\n",
    "    if kriter == \"vram\":\n",
    "        m = re.search(r'vram\\s*(\\d+)\\s*gb', prompt)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    if kriter == \"cuda\":\n",
    "        m = re.search(r'cuda\\s*\\d+\\s*Ã§ekirdek', prompt)\n",
    "        if m:\n",
    "            return int(re.search(r'(\\d+)', m.group()).group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def performans_belirle(prompt):\n",
    "    pl = prompt.lower()\n",
    "    if \"Ã§ok yÃ¼ksek\" in pl:\n",
    "        return \"Ã§ok yÃ¼ksek\"\n",
    "    if \"yÃ¼ksek\" in pl:\n",
    "        return \"yÃ¼ksek\"\n",
    "    if \"orta\" in pl:\n",
    "        return \"orta\"\n",
    "    if \"dÃ¼ÅŸÃ¼k\" in pl:\n",
    "        return \"dÃ¼ÅŸÃ¼k\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def kullanÄ±m_alani_belirle(prompt):\n",
    "    alanlar = [\"oyun\", \"render\", \"ofis\", \"ai\", \"yapay zeka\"]\n",
    "    for a in alanlar:\n",
    "        if a in prompt.lower():\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# GPU Ã¶neri fonksiyonu\n",
    "def gpu_oner(prompt):\n",
    "    p = prompt.lower()\n",
    "    # Filtre kriterleri\n",
    "    min_vram = sayisal_aralik_belirle(p, \"vram\") or 0\n",
    "    min_cuda = sayisal_aralik_belirle(p, \"cuda\") or 0\n",
    "    perf = performans_belirle(p)\n",
    "    alan = kullanÄ±m_alani_belirle(p)\n",
    "\n",
    "    df_filt = df.copy()\n",
    "    if min_vram:\n",
    "        df_filt = df_filt[df_filt[\"VRAM (GB)\"] >= min_vram]\n",
    "    if min_cuda:\n",
    "        df_filt = df_filt[df_filt[\"CUDA Ã‡ekirdekleri\"] >= min_cuda]\n",
    "    if perf:\n",
    "        df_filt = df_filt[df_filt[\"Performans\"] == perf]\n",
    "    if alan:\n",
    "        df_filt = df_filt[df_filt[\"KullanÄ±m AlanlarÄ±\"].str.lower().str.contains(alan)]\n",
    "\n",
    "    if df_filt.empty:\n",
    "        return \"ğŸ¤” Bu kriterlere uygun GPU bulunamadÄ±. LÃ¼tfen daha detaylÄ± bilgi verin.\"\n",
    "\n",
    "    # Normalize sayÄ±sal\n",
    "    df_filt = df_filt.copy()\n",
    "    df_filt['n_vram'] = normalize(df_filt['VRAM (GB)'])\n",
    "    df_filt['n_cuda'] = normalize(df_filt['CUDA Ã‡ekirdekleri'])\n",
    "    df_filt['n_band'] = normalize(df_filt['Bellek Bant GeniÅŸliÄŸi (GB/s)'])\n",
    "\n",
    "    # Embedding + sayÄ±sal vektÃ¶r\n",
    "    text_embeds = embed_model.encode(df_filt['etiket_birlesik'].tolist())\n",
    "    nums = df_filt[['n_vram','n_cuda','n_band']].values\n",
    "    combined = np.hstack([text_embeds, nums])\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "    prompt_ext = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    sims = cosine_similarity([prompt_ext], combined)[0]\n",
    "    idx = np.argmax(sims)\n",
    "    gpu = df_filt.iloc[idx]\n",
    "\n",
    "    # Agent ile workflow kaydÄ±\n",
    "    wf = Workflow(text=prompt, client_mode=False)\n",
    "    wf.custom(\n",
    "        name=\"gpu_oner_task\",\n",
    "        objective=f\"Prompt: {prompt} - Ã–nerilen GPU: {gpu['GPU Modeli']}\",\n",
    "        instructions=f\"KullanÄ±cÄ± isteÄŸi: {prompt} | Ã–nerilen: {gpu['GPU Modeli']}\",\n",
    "        agents=[agent]\n",
    "    )\n",
    "\n",
    "    return f\"ğŸ¯ En uygun GPU: {gpu['GPU Modeli']} (VRAM: {gpu['VRAM (GB)']}GB, CUDA: {gpu['CUDA Ã‡ekirdekleri']}, Performans: {gpu['Performans']})\"\n",
    "\n",
    "# Sohbet dÃ¶ngÃ¼sÃ¼\n",
    "conv = []\n",
    "print(\"GPU AsistanÄ± hazÄ±r! 'exit' diyene kadar devam.\")\n",
    "while True:\n",
    "    u = input(\"Sen: \")\n",
    "    if u.lower()=='exit': break\n",
    "    res = gpu_oner(u)\n",
    "    conv.append({'role':'user','content':u})\n",
    "    conv.append({'role':'assistant','content':res})\n",
    "    print(\"Bot:\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22845a3e-65cf-4bd3-a411-fff76cd91df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU AsistanÄ± hazÄ±r! 'exit' diyene kadar devam.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  profesyonel uygulamalar iÃ§in Ã§ok yÃ¼ksek performansa ihtiyacÄ±m var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Ã–nerilen GPU'lar:\n",
      "RTX A6000 (VRAM: 48GB, CUDA: 10752.0, Performans: Ã§ok yÃ¼ksek)\n",
      "GeForce RTX 3090 Ti (VRAM: 24GB, CUDA: 10752.0, Performans: Ã§ok yÃ¼ksek)\n",
      "A100-PCIE-40GB (VRAM: 40GB, CUDA: 6912.0, Performans: Ã§ok yÃ¼ksek)\n",
      "GeForce RTX 3090 (VRAM: 24GB, CUDA: 10496.0, Performans: Ã§ok yÃ¼ksek)\n",
      "GeForce RTX 3080 Ti (VRAM: 12GB, CUDA: 10240.0, Performans: Ã§ok yÃ¼ksek)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  oyun iÃ§in orta performansta hangi gpularÄ± Ã¶nerirsin \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ğŸ¯ Ã–nerilen GPU'lar:\n",
      "GeForce RTX 3060 (VRAM: 12GB, CUDA: 3584.0, Performans: orta)\n",
      "GeForce RTX 4060 (VRAM: 8GB, CUDA: 3072.0, Performans: orta)\n",
      "GeForce RTX 4060 Ti (VRAM: 8GB, CUDA: 4352.0, Performans: orta)\n",
      "GeForce GTX 1080 Ti (VRAM: 11GB, CUDA: 3584.0, Performans: orta)\n",
      "GeForce RTX 3060 Ti (VRAM: 8GB, CUDA: 4864.0, Performans: orta)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Sen:  exit\n"
     ]
    }
   ],
   "source": [
    "from iointel import Agent, Workflow\n",
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# API AnahtarÄ±nÄ± ayarla\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"io-v2-eyJhbGciOi...\"  # GerÃ§ek IO.net API anahtarÄ±nÄ± buraya gir\n",
    "\n",
    "# CSV verisini oku\n",
    "df = pd.read_csv(r\"C:\\Users\\sevva\\OneDrive\\MasaÃ¼stÃ¼\\gpu_listesi_duzenlenmis.csv\")\n",
    "\n",
    "# PerformansÄ± ve kullanÄ±m alanÄ±nÄ± birleÅŸtirerek metin oluÅŸtur\n",
    "df[\"Performans\"] = df[\"Performans\"].str.strip().str.lower()\n",
    "df[\"etiket_birlesik\"] = df.apply(\n",
    "    lambda r: f\"{r['GPU Modeli']} {r['KullanÄ±m AlanlarÄ±']} {r['Performans']} {r['VRAM (GB)']}GB {r['CUDA Ã‡ekirdekleri']} Ã§ekirdek {r['Bellek Bant GeniÅŸliÄŸi (GB/s)']}GB/s\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# NaN deÄŸerleri doldur\n",
    "df.ffill(inplace=True)\n",
    "\n",
    "# Embedding modeli yÃ¼kle\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# IO.net Agent oluÅŸtur\n",
    "agent = Agent(\n",
    "    name=\"GPU Ã–nerici Agent\",\n",
    "    instructions=\"KullanÄ±cÄ±nÄ±n ihtiyaÃ§larÄ±na gÃ¶re uygun GPU Ã¶ner. Gerekirse ek bilgi iste.\",\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=\"https://api.intelligence.io.solutions/api/v1\"\n",
    ")\n",
    "\n",
    "# YardÄ±mcÄ± fonksiyonlar\n",
    "def normalize(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "def sayisal_aralik_belirle(prompt, kriter):\n",
    "    if kriter == \"vram\":\n",
    "        m = re.search(r'vram\\s*(\\d+)\\s*gb', prompt)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "    if kriter == \"cuda\":\n",
    "        m = re.search(r'cuda\\s*\\d+\\s*Ã§ekirdek', prompt)\n",
    "        if m:\n",
    "            return int(re.search(r'(\\d+)', m.group()).group(1))\n",
    "    return None\n",
    "\n",
    "def performans_belirle(prompt):\n",
    "    pl = prompt.lower()\n",
    "    if \"Ã§ok yÃ¼ksek\" in pl:\n",
    "        return \"Ã§ok yÃ¼ksek\"\n",
    "    if \"yÃ¼ksek\" in pl:\n",
    "        return \"yÃ¼ksek\"\n",
    "    if \"orta\" in pl:\n",
    "        return \"orta\"\n",
    "    if \"dÃ¼ÅŸÃ¼k\" in pl:\n",
    "        return \"dÃ¼ÅŸÃ¼k\"\n",
    "    return None\n",
    "\n",
    "def kullanÄ±m_alani_belirle(prompt):\n",
    "    alanlar = [\"oyun\", \"render\", \"ofis\", \"ai\", \"yapay zeka\"]\n",
    "    for a in alanlar:\n",
    "        if a in prompt.lower():\n",
    "            return a\n",
    "    return None\n",
    "\n",
    "# GPU Ã¶neri fonksiyonu\n",
    "def gpu_oner(prompt, max_results=5):\n",
    "    p = prompt.lower()\n",
    "    min_vram = sayisal_aralik_belirle(p, \"vram\") or 0\n",
    "    min_cuda = sayisal_aralik_belirle(p, \"cuda\") or 0\n",
    "    perf = performans_belirle(p)\n",
    "    alan = kullanÄ±m_alani_belirle(p)\n",
    "\n",
    "    df_filt = df.copy()\n",
    "    if min_vram:\n",
    "        df_filt = df_filt[df_filt[\"VRAM (GB)\"] >= min_vram]\n",
    "    if min_cuda:\n",
    "        df_filt = df_filt[df_filt[\"CUDA Ã‡ekirdekleri\"] >= min_cuda]\n",
    "    if perf:\n",
    "        df_filt = df_filt[df_filt[\"Performans\"] == perf]\n",
    "    if alan:\n",
    "        df_filt = df_filt[df_filt[\"KullanÄ±m AlanlarÄ±\"].str.lower().str.contains(alan)]\n",
    "\n",
    "    if df_filt.empty:\n",
    "        return \"ğŸ¤” Bu kriterlere uygun GPU bulunamadÄ±. LÃ¼tfen daha detaylÄ± bilgi verin.\"\n",
    "\n",
    "    df_filt.ffill(inplace=True)\n",
    "\n",
    "    df_filt['n_vram'] = normalize(df_filt['VRAM (GB)'])\n",
    "    df_filt['n_cuda'] = normalize(df_filt['CUDA Ã‡ekirdekleri'])\n",
    "    df_filt['n_band'] = normalize(df_filt['Bellek Bant GeniÅŸliÄŸi (GB/s)'])\n",
    "\n",
    "    text_embeds = embed_model.encode(df_filt['etiket_birlesik'].tolist())\n",
    "    nums = df_filt[['n_vram','n_cuda','n_band']].values\n",
    "    combined = np.hstack([text_embeds, nums])\n",
    "    prompt_embed = embed_model.encode([prompt])[0]\n",
    "    prompt_ext = np.concatenate([prompt_embed, np.zeros(3)])\n",
    "\n",
    "    sims = cosine_similarity([prompt_ext], combined)[0]\n",
    "    idx = np.argsort(sims)[-max_results:][::-1]\n",
    "\n",
    "    gpu_list = []\n",
    "    for i in idx:\n",
    "        gpu = df_filt.iloc[i]\n",
    "        gpu_list.append(f\"{gpu['GPU Modeli']} (VRAM: {gpu['VRAM (GB)']}GB, CUDA: {gpu['CUDA Ã‡ekirdekleri']}, Performans: {gpu['Performans']})\")\n",
    "\n",
    "    # Agent ile kayÄ±t\n",
    "    wf = Workflow(text=prompt, client_mode=False)\n",
    "    wf.custom(\n",
    "        name=\"gpu_oner_task\",\n",
    "        objective=f\"Prompt: {prompt} - Ã–nerilen GPU'lar:\\n\" + \"\\n\".join(gpu_list),\n",
    "        instructions=f\"KullanÄ±cÄ± isteÄŸi: {prompt} | Ã–nerilenler:\\n\" + \"\\n\".join(gpu_list),\n",
    "        agents=[agent]\n",
    "    )\n",
    "\n",
    "    return f\"ğŸ¯ Ã–nerilen GPU'lar:\\n\" + \"\\n\".join(gpu_list)\n",
    "\n",
    "# Sohbet baÅŸlat\n",
    "conv = []\n",
    "print(\"GPU AsistanÄ± hazÄ±r! 'exit' diyene kadar devam.\")\n",
    "while True:\n",
    "    u = input(\"Sen: \")\n",
    "    if u.lower() == 'exit':\n",
    "        break\n",
    "    res = gpu_oner(u)\n",
    "    conv.append({'role': 'user', 'content': u})\n",
    "    conv.append({'role': 'assistant', 'content': res})\n",
    "    print(\"Bot:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb6e64-20d7-4eee-9a9e-542893d65a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
